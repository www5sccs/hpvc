#ifndef _ROTATINGHEATSOURCE_EXPLICITEULER_RECORDS_VERTEX_H
#define _ROTATINGHEATSOURCE_EXPLICITEULER_RECORDS_VERTEX_H

#include "peano/utils/Globals.h"
#include "tarch/compiler/CompilerSpecificSettings.h"
#include "peano/utils/PeanoOptimisations.h"
#ifdef Parallel
	#include "tarch/parallel/Node.h"
#endif
#ifdef Parallel
	#include <mpi.h>
#endif
#include "tarch/logging/Log.h"
#include "tarch/la/Vector.h"
#include <bitset>
#include <complex>
#include <string>
#include <iostream>

namespace rotatingheatsource {
   namespace expliciteuler {
      namespace records {
         class Vertex;
         class VertexPacked;
      }
   }
}

#if defined(Parallel) && defined(Asserts)
   /**
    * @author This class is generated by DaStGen
    * 		   DataStructureGenerator (DaStGen)
    * 		   2007-2009 Wolfgang Eckhardt
    * 		   2012      Tobias Weinzierl
    *
    * 		   build date: 20-02-2013 11:21
    *
    * @date   02/04/2013 15:53
    */
   class rotatingheatsource::expliciteuler::records::Vertex { 
      
      public:
         
         typedef rotatingheatsource::expliciteuler::records::VertexPacked Packed;
         
         enum InsideOutsideDomain {
            Inside = 0, Boundary = 1, Outside = 2
         };
         
         enum RefinementControl {
            Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6
         };
         
         struct PersistentRecords {
            double _rhs;
            double _u;
            #ifdef UseManualAlignment
            tarch::la::Vector<THREE_POWER_D,double> _stencil __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<THREE_POWER_D,double> _stencil;
            #endif
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _linearSurplus __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _linearSurplus;
            #endif
            bool _isHangingNode;
            RefinementControl _refinementControl;
            int _adjacentCellsHeight;
            InsideOutsideDomain _insideOutsideDomain;
            #ifdef UseManualAlignment
            tarch::la::Vector<DIMENSIONS,double> _x __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<DIMENSIONS,double> _x;
            #endif
            int _level;
            #ifdef UseManualAlignment
            tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks __attribute__((aligned(VectorisationAlignment)));
            #else
            tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
            #endif
            /**
             * Generated
             */
            PersistentRecords();
            
            /**
             * Generated
             */
            PersistentRecords(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
            
            
            inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _rhs;
            }
            
            
            
            inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _rhs = rhs;
            }
            
            
            
            inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _u;
            }
            
            
            
            inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _u = u;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _stencil;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _stencil = (stencil);
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _linearSurplus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _linearSurplus = (linearSurplus);
            }
            
            
            
            inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _isHangingNode;
            }
            
            
            
            inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _isHangingNode = isHangingNode;
            }
            
            
            
            inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _refinementControl;
            }
            
            
            
            inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _refinementControl = refinementControl;
            }
            
            
            
            inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentCellsHeight;
            }
            
            
            
            inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentCellsHeight = adjacentCellsHeight;
            }
            
            
            
            inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _insideOutsideDomain;
            }
            
            
            
            inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _insideOutsideDomain = insideOutsideDomain;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _x;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _x = (x);
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentRanks;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentRanks = (adjacentRanks);
            }
            
            
            
         };
         
      private: 
         PersistentRecords _persistentRecords;
         double _residual;
         int _adjacentCellsHeightOfPreviousIteration;
         int _numberOfAdjacentRefinedCells;
         
      public:
         /**
          * Generated
          */
         Vertex();
         
         /**
          * Generated
          */
         Vertex(const PersistentRecords& persistentRecords);
         
         /**
          * Generated
          */
         Vertex(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
         
         /**
          * Generated
          */
         Vertex(const double& rhs, const double& u, const double& residual, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
         
         /**
          * Generated
          */
         virtual ~Vertex();
         
         
         inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._rhs;
         }
         
         
         
         inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._rhs = rhs;
         }
         
         
         
         inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._u;
         }
         
         
         
         inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._u = u;
         }
         
         
         
         inline double getResidual() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _residual;
         }
         
         
         
         inline void setResidual(const double& residual) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _residual = residual;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._stencil;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._stencil = (stencil);
         }
         
         
         
         inline double getStencil(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<THREE_POWER_D);
            return _persistentRecords._stencil[elementIndex];
            
         }
         
         
         
         inline void setStencil(int elementIndex, const double& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<THREE_POWER_D);
            _persistentRecords._stencil[elementIndex]= stencil;
            
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._linearSurplus;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._linearSurplus = (linearSurplus);
         }
         
         
         
         inline double getLinearSurplus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._linearSurplus[elementIndex];
            
         }
         
         
         
         inline void setLinearSurplus(int elementIndex, const double& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._linearSurplus[elementIndex]= linearSurplus;
            
         }
         
         
         
         inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._isHangingNode;
         }
         
         
         
         inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._isHangingNode = isHangingNode;
         }
         
         
         
         inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._refinementControl;
         }
         
         
         
         inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._refinementControl = refinementControl;
         }
         
         
         
         inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._adjacentCellsHeight;
         }
         
         
         
         inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
         }
         
         
         
         inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _adjacentCellsHeightOfPreviousIteration;
         }
         
         
         
         inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
         }
         
         
         
         inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _numberOfAdjacentRefinedCells;
         }
         
         
         
         inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
         }
         
         
         
         inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._insideOutsideDomain;
         }
         
         
         
         inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._insideOutsideDomain = insideOutsideDomain;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._x;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._x = (x);
         }
         
         
         
         inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            return _persistentRecords._x[elementIndex];
            
         }
         
         
         
         inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<DIMENSIONS);
            _persistentRecords._x[elementIndex]= x;
            
         }
         
         
         
         inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._level;
         }
         
         
         
         inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._level = level;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            return _persistentRecords._adjacentRanks;
         }
         
         
         
         /**
          * Generated and optimized
          * 
          * If you realise a for loop using exclusively arrays (vectors) and compile 
          * with -DUseManualAlignment you may add 
          * \code
          #pragma vector aligned
          #pragma simd
          \endcode to this for loop to enforce your compiler to use SSE/AVX.
          * 
          * The alignment is tied to the unpacked records, i.e. for packed class
          * variants the machine's natural alignment is switched off to recude the  
          * memory footprint. Do not use any SSE/AVX operations or 
          * vectorisation on the result for the packed variants, as the data is misaligned. 
          * If you rely on vectorisation, convert the underlying record 
          * into the unpacked version first. 
          * 
          * @see convert()
          */
         inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            _persistentRecords._adjacentRanks = (adjacentRanks);
         }
         
         
         
         inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<TWO_POWER_D);
            return _persistentRecords._adjacentRanks[elementIndex];
            
         }
         
         
         
         inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
            assertion(elementIndex>=0);
            assertion(elementIndex<TWO_POWER_D);
            _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
            
         }
         
         
         /**
          * Generated
          */
         static std::string toString(const InsideOutsideDomain& param);
         
         /**
          * Generated
          */
         static std::string getInsideOutsideDomainMapping();
         
         /**
          * Generated
          */
         static std::string toString(const RefinementControl& param);
         
         /**
          * Generated
          */
         static std::string getRefinementControlMapping();
         
         /**
          * Generated
          */
         std::string toString() const;
         
         /**
          * Generated
          */
         void toString(std::ostream& out) const;
         
         
         PersistentRecords getPersistentRecords() const;
         /**
          * Generated
          */
         VertexPacked convert() const;
         
         
      #ifdef Parallel
         protected:
            static tarch::logging::Log _log;
            
            int _senderDestinationRank;
            
         public:
            
            /**
             * Global that represents the mpi datatype.
             * There are two variants: Datatype identifies only those attributes marked with
             * parallelise. FullDatatype instead identifies the whole record with all fields.
             */
            static MPI_Datatype Datatype;
            static MPI_Datatype FullDatatype;
            
            /**
             * Initializes the data type for the mpi operations. Has to be called
             * before the very first send or receive operation is called.
             */
            static void initDatatype();
            
            static void shutdownDatatype();
            
            void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
            
            int getSenderRank() const;
            
      #endif
         
      };
      
      #ifndef DaStGenPackedPadding
        #define DaStGenPackedPadding 1      // 32 bit version
        // #define DaStGenPackedPadding 2   // 64 bit version
      #endif
      
      
      #ifdef PackedRecords
         #pragma pack (push, DaStGenPackedPadding)
      #endif
      
      /**
       * @author This class is generated by DaStGen
       * 		   DataStructureGenerator (DaStGen)
       * 		   2007-2009 Wolfgang Eckhardt
       * 		   2012      Tobias Weinzierl
       *
       * 		   build date: 20-02-2013 11:21
       *
       * @date   02/04/2013 15:53
       */
      class rotatingheatsource::expliciteuler::records::VertexPacked { 
         
         public:
            
            typedef rotatingheatsource::expliciteuler::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
            
            typedef rotatingheatsource::expliciteuler::records::Vertex::RefinementControl RefinementControl;
            
            struct PersistentRecords {
               double _rhs;
               double _u;
               tarch::la::Vector<THREE_POWER_D,double> _stencil;
               tarch::la::Vector<DIMENSIONS,double> _linearSurplus;
               int _adjacentCellsHeight;
               tarch::la::Vector<DIMENSIONS,double> _x;
               int _level;
               tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
               
               /** mapping of records:
               || Member 	|| startbit 	|| length
                |  isHangingNode	| startbit 0	| #bits 1
                |  refinementControl	| startbit 1	| #bits 3
                |  insideOutsideDomain	| startbit 4	| #bits 2
                */
               short int _packedRecords0;
               
               /**
                * Generated
                */
               PersistentRecords();
               
               /**
                * Generated
                */
               PersistentRecords(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
               
               
               inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _rhs;
               }
               
               
               
               inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _rhs = rhs;
               }
               
               
               
               inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _u;
               }
               
               
               
               inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _u = u;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _stencil;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _stencil = (stencil);
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _linearSurplus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _linearSurplus = (linearSurplus);
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | refinementControl << (1));
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | insideOutsideDomain << (4));
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _x;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _x = (x);
               }
               
               
               
               inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _level;
               }
               
               
               
               inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _level = level;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentRanks;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentRanks = (adjacentRanks);
               }
               
               
               
            };
            
         private: 
            PersistentRecords _persistentRecords;
            double _residual;
            int _adjacentCellsHeightOfPreviousIteration;
            int _numberOfAdjacentRefinedCells;
            
         public:
            /**
             * Generated
             */
            VertexPacked();
            
            /**
             * Generated
             */
            VertexPacked(const PersistentRecords& persistentRecords);
            
            /**
             * Generated
             */
            VertexPacked(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
            
            /**
             * Generated
             */
            VertexPacked(const double& rhs, const double& u, const double& residual, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
            
            /**
             * Generated
             */
            virtual ~VertexPacked();
            
            
            inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._rhs;
            }
            
            
            
            inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._rhs = rhs;
            }
            
            
            
            inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._u;
            }
            
            
            
            inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._u = u;
            }
            
            
            
            inline double getResidual() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _residual;
            }
            
            
            
            inline void setResidual(const double& residual) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _residual = residual;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._stencil;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._stencil = (stencil);
            }
            
            
            
            inline double getStencil(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<THREE_POWER_D);
               return _persistentRecords._stencil[elementIndex];
               
            }
            
            
            
            inline void setStencil(int elementIndex, const double& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<THREE_POWER_D);
               _persistentRecords._stencil[elementIndex]= stencil;
               
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._linearSurplus;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._linearSurplus = (linearSurplus);
            }
            
            
            
            inline double getLinearSurplus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._linearSurplus[elementIndex];
               
            }
            
            
            
            inline void setLinearSurplus(int elementIndex, const double& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._linearSurplus[elementIndex]= linearSurplus;
               
            }
            
            
            
            inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
            }
            
            
            
            inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
            }
            
            
            
            inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
            }
            
            
            
            inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | refinementControl << (1));
            }
            
            
            
            inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._adjacentCellsHeight;
            }
            
            
            
            inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
            }
            
            
            
            inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _adjacentCellsHeightOfPreviousIteration;
            }
            
            
            
            inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
            }
            
            
            
            inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _numberOfAdjacentRefinedCells;
            }
            
            
            
            inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
            }
            
            
            
            inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
            }
            
            
            
            inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | insideOutsideDomain << (4));
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._x;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._x = (x);
            }
            
            
            
            inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               return _persistentRecords._x[elementIndex];
               
            }
            
            
            
            inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<DIMENSIONS);
               _persistentRecords._x[elementIndex]= x;
               
            }
            
            
            
            inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._level;
            }
            
            
            
            inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._level = level;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               return _persistentRecords._adjacentRanks;
            }
            
            
            
            /**
             * Generated and optimized
             * 
             * If you realise a for loop using exclusively arrays (vectors) and compile 
             * with -DUseManualAlignment you may add 
             * \code
             #pragma vector aligned
             #pragma simd
             \endcode to this for loop to enforce your compiler to use SSE/AVX.
             * 
             * The alignment is tied to the unpacked records, i.e. for packed class
             * variants the machine's natural alignment is switched off to recude the  
             * memory footprint. Do not use any SSE/AVX operations or 
             * vectorisation on the result for the packed variants, as the data is misaligned. 
             * If you rely on vectorisation, convert the underlying record 
             * into the unpacked version first. 
             * 
             * @see convert()
             */
            inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               _persistentRecords._adjacentRanks = (adjacentRanks);
            }
            
            
            
            inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               return _persistentRecords._adjacentRanks[elementIndex];
               
            }
            
            
            
            inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
               assertion(elementIndex>=0);
               assertion(elementIndex<TWO_POWER_D);
               _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
               
            }
            
            
            /**
             * Generated
             */
            static std::string toString(const InsideOutsideDomain& param);
            
            /**
             * Generated
             */
            static std::string getInsideOutsideDomainMapping();
            
            /**
             * Generated
             */
            static std::string toString(const RefinementControl& param);
            
            /**
             * Generated
             */
            static std::string getRefinementControlMapping();
            
            /**
             * Generated
             */
            std::string toString() const;
            
            /**
             * Generated
             */
            void toString(std::ostream& out) const;
            
            
            PersistentRecords getPersistentRecords() const;
            /**
             * Generated
             */
            Vertex convert() const;
            
            
         #ifdef Parallel
            protected:
               static tarch::logging::Log _log;
               
               int _senderDestinationRank;
               
            public:
               
               /**
                * Global that represents the mpi datatype.
                * There are two variants: Datatype identifies only those attributes marked with
                * parallelise. FullDatatype instead identifies the whole record with all fields.
                */
               static MPI_Datatype Datatype;
               static MPI_Datatype FullDatatype;
               
               /**
                * Initializes the data type for the mpi operations. Has to be called
                * before the very first send or receive operation is called.
                */
               static void initDatatype();
               
               static void shutdownDatatype();
               
               void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
               
               int getSenderRank() const;
               
         #endif
            
         };
         
         #ifdef PackedRecords
         #pragma pack (pop)
         #endif
         
         
         
      #elif !defined(Parallel) && !defined(Asserts)
         /**
          * @author This class is generated by DaStGen
          * 		   DataStructureGenerator (DaStGen)
          * 		   2007-2009 Wolfgang Eckhardt
          * 		   2012      Tobias Weinzierl
          *
          * 		   build date: 20-02-2013 11:21
          *
          * @date   02/04/2013 15:53
          */
         class rotatingheatsource::expliciteuler::records::Vertex { 
            
            public:
               
               typedef rotatingheatsource::expliciteuler::records::VertexPacked Packed;
               
               enum InsideOutsideDomain {
                  Inside = 0, Boundary = 1, Outside = 2
               };
               
               enum RefinementControl {
                  Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6
               };
               
               struct PersistentRecords {
                  double _rhs;
                  double _u;
                  #ifdef UseManualAlignment
                  tarch::la::Vector<THREE_POWER_D,double> _stencil __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<THREE_POWER_D,double> _stencil;
                  #endif
                  #ifdef UseManualAlignment
                  tarch::la::Vector<DIMENSIONS,double> _linearSurplus __attribute__((aligned(VectorisationAlignment)));
                  #else
                  tarch::la::Vector<DIMENSIONS,double> _linearSurplus;
                  #endif
                  bool _isHangingNode;
                  RefinementControl _refinementControl;
                  int _adjacentCellsHeight;
                  InsideOutsideDomain _insideOutsideDomain;
                  /**
                   * Generated
                   */
                  PersistentRecords();
                  
                  /**
                   * Generated
                   */
                  PersistentRecords(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
                  
                  
                  inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _rhs;
                  }
                  
                  
                  
                  inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _rhs = rhs;
                  }
                  
                  
                  
                  inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _u;
                  }
                  
                  
                  
                  inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _u = u;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _stencil;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _stencil = (stencil);
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _linearSurplus;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _linearSurplus = (linearSurplus);
                  }
                  
                  
                  
                  inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _isHangingNode;
                  }
                  
                  
                  
                  inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _isHangingNode = isHangingNode;
                  }
                  
                  
                  
                  inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _refinementControl;
                  }
                  
                  
                  
                  inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _refinementControl = refinementControl;
                  }
                  
                  
                  
                  inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _adjacentCellsHeight;
                  }
                  
                  
                  
                  inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _adjacentCellsHeight = adjacentCellsHeight;
                  }
                  
                  
                  
                  inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _insideOutsideDomain;
                  }
                  
                  
                  
                  inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _insideOutsideDomain = insideOutsideDomain;
                  }
                  
                  
                  
               };
               
            private: 
               PersistentRecords _persistentRecords;
               double _residual;
               int _adjacentCellsHeightOfPreviousIteration;
               int _numberOfAdjacentRefinedCells;
               
            public:
               /**
                * Generated
                */
               Vertex();
               
               /**
                * Generated
                */
               Vertex(const PersistentRecords& persistentRecords);
               
               /**
                * Generated
                */
               Vertex(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
               
               /**
                * Generated
                */
               Vertex(const double& rhs, const double& u, const double& residual, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain);
               
               /**
                * Generated
                */
               virtual ~Vertex();
               
               
               inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._rhs;
               }
               
               
               
               inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._rhs = rhs;
               }
               
               
               
               inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._u;
               }
               
               
               
               inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._u = u;
               }
               
               
               
               inline double getResidual() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _residual;
               }
               
               
               
               inline void setResidual(const double& residual) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _residual = residual;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._stencil;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._stencil = (stencil);
               }
               
               
               
               inline double getStencil(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<THREE_POWER_D);
                  return _persistentRecords._stencil[elementIndex];
                  
               }
               
               
               
               inline void setStencil(int elementIndex, const double& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<THREE_POWER_D);
                  _persistentRecords._stencil[elementIndex]= stencil;
                  
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._linearSurplus;
               }
               
               
               
               /**
                * Generated and optimized
                * 
                * If you realise a for loop using exclusively arrays (vectors) and compile 
                * with -DUseManualAlignment you may add 
                * \code
                #pragma vector aligned
                #pragma simd
                \endcode to this for loop to enforce your compiler to use SSE/AVX.
                * 
                * The alignment is tied to the unpacked records, i.e. for packed class
                * variants the machine's natural alignment is switched off to recude the  
                * memory footprint. Do not use any SSE/AVX operations or 
                * vectorisation on the result for the packed variants, as the data is misaligned. 
                * If you rely on vectorisation, convert the underlying record 
                * into the unpacked version first. 
                * 
                * @see convert()
                */
               inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._linearSurplus = (linearSurplus);
               }
               
               
               
               inline double getLinearSurplus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  return _persistentRecords._linearSurplus[elementIndex];
                  
               }
               
               
               
               inline void setLinearSurplus(int elementIndex, const double& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  assertion(elementIndex>=0);
                  assertion(elementIndex<DIMENSIONS);
                  _persistentRecords._linearSurplus[elementIndex]= linearSurplus;
                  
               }
               
               
               
               inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._isHangingNode;
               }
               
               
               
               inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._isHangingNode = isHangingNode;
               }
               
               
               
               inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._refinementControl;
               }
               
               
               
               inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._refinementControl = refinementControl;
               }
               
               
               
               inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._adjacentCellsHeight;
               }
               
               
               
               inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
               }
               
               
               
               inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
               }
               
               
               
               inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _numberOfAdjacentRefinedCells;
               }
               
               
               
               inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
               }
               
               
               
               inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  return _persistentRecords._insideOutsideDomain;
               }
               
               
               
               inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                  _persistentRecords._insideOutsideDomain = insideOutsideDomain;
               }
               
               
               /**
                * Generated
                */
               static std::string toString(const InsideOutsideDomain& param);
               
               /**
                * Generated
                */
               static std::string getInsideOutsideDomainMapping();
               
               /**
                * Generated
                */
               static std::string toString(const RefinementControl& param);
               
               /**
                * Generated
                */
               static std::string getRefinementControlMapping();
               
               /**
                * Generated
                */
               std::string toString() const;
               
               /**
                * Generated
                */
               void toString(std::ostream& out) const;
               
               
               PersistentRecords getPersistentRecords() const;
               /**
                * Generated
                */
               VertexPacked convert() const;
               
               
            #ifdef Parallel
               protected:
                  static tarch::logging::Log _log;
                  
                  int _senderDestinationRank;
                  
               public:
                  
                  /**
                   * Global that represents the mpi datatype.
                   * There are two variants: Datatype identifies only those attributes marked with
                   * parallelise. FullDatatype instead identifies the whole record with all fields.
                   */
                  static MPI_Datatype Datatype;
                  static MPI_Datatype FullDatatype;
                  
                  /**
                   * Initializes the data type for the mpi operations. Has to be called
                   * before the very first send or receive operation is called.
                   */
                  static void initDatatype();
                  
                  static void shutdownDatatype();
                  
                  void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                  
                  int getSenderRank() const;
                  
            #endif
               
            };
            
            #ifndef DaStGenPackedPadding
              #define DaStGenPackedPadding 1      // 32 bit version
              // #define DaStGenPackedPadding 2   // 64 bit version
            #endif
            
            
            #ifdef PackedRecords
               #pragma pack (push, DaStGenPackedPadding)
            #endif
            
            /**
             * @author This class is generated by DaStGen
             * 		   DataStructureGenerator (DaStGen)
             * 		   2007-2009 Wolfgang Eckhardt
             * 		   2012      Tobias Weinzierl
             *
             * 		   build date: 20-02-2013 11:21
             *
             * @date   02/04/2013 15:53
             */
            class rotatingheatsource::expliciteuler::records::VertexPacked { 
               
               public:
                  
                  typedef rotatingheatsource::expliciteuler::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
                  
                  typedef rotatingheatsource::expliciteuler::records::Vertex::RefinementControl RefinementControl;
                  
                  struct PersistentRecords {
                     double _rhs;
                     double _u;
                     tarch::la::Vector<THREE_POWER_D,double> _stencil;
                     tarch::la::Vector<DIMENSIONS,double> _linearSurplus;
                     int _adjacentCellsHeight;
                     
                     /** mapping of records:
                     || Member 	|| startbit 	|| length
                      |  isHangingNode	| startbit 0	| #bits 1
                      |  refinementControl	| startbit 1	| #bits 3
                      |  insideOutsideDomain	| startbit 4	| #bits 2
                      */
                     short int _packedRecords0;
                     
                     /**
                      * Generated
                      */
                     PersistentRecords();
                     
                     /**
                      * Generated
                      */
                     PersistentRecords(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
                     
                     
                     inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _rhs;
                     }
                     
                     
                     
                     inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _rhs = rhs;
                     }
                     
                     
                     
                     inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _u;
                     }
                     
                     
                     
                     inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _u = u;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _stencil;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _stencil = (stencil);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _linearSurplus;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _linearSurplus = (linearSurplus);
                     }
                     
                     
                     
                     inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                     }
                     
                     
                     
                     inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | refinementControl << (1));
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _adjacentCellsHeight;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _adjacentCellsHeight = adjacentCellsHeight;
                     }
                     
                     
                     
                     inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                     }
                     
                     
                     
                     inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | insideOutsideDomain << (4));
                     }
                     
                     
                     
                  };
                  
               private: 
                  PersistentRecords _persistentRecords;
                  double _residual;
                  int _adjacentCellsHeightOfPreviousIteration;
                  int _numberOfAdjacentRefinedCells;
                  
               public:
                  /**
                   * Generated
                   */
                  VertexPacked();
                  
                  /**
                   * Generated
                   */
                  VertexPacked(const PersistentRecords& persistentRecords);
                  
                  /**
                   * Generated
                   */
                  VertexPacked(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain);
                  
                  /**
                   * Generated
                   */
                  VertexPacked(const double& rhs, const double& u, const double& residual, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain);
                  
                  /**
                   * Generated
                   */
                  virtual ~VertexPacked();
                  
                  
                  inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._rhs;
                  }
                  
                  
                  
                  inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._rhs = rhs;
                  }
                  
                  
                  
                  inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._u;
                  }
                  
                  
                  
                  inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._u = u;
                  }
                  
                  
                  
                  inline double getResidual() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _residual;
                  }
                  
                  
                  
                  inline void setResidual(const double& residual) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _residual = residual;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._stencil;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._stencil = (stencil);
                  }
                  
                  
                  
                  inline double getStencil(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<THREE_POWER_D);
                     return _persistentRecords._stencil[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setStencil(int elementIndex, const double& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<THREE_POWER_D);
                     _persistentRecords._stencil[elementIndex]= stencil;
                     
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._linearSurplus;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._linearSurplus = (linearSurplus);
                  }
                  
                  
                  
                  inline double getLinearSurplus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._linearSurplus[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setLinearSurplus(int elementIndex, const double& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._linearSurplus[elementIndex]= linearSurplus;
                     
                  }
                  
                  
                  
                  inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                  }
                  
                  
                  
                  inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                  }
                  
                  
                  
                  inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                  }
                  
                  
                  
                  inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | refinementControl << (1));
                  }
                  
                  
                  
                  inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._adjacentCellsHeight;
                  }
                  
                  
                  
                  inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
                  }
                  
                  
                  
                  inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _adjacentCellsHeightOfPreviousIteration;
                  }
                  
                  
                  
                  inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
                  }
                  
                  
                  
                  inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfAdjacentRefinedCells;
                  }
                  
                  
                  
                  inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
                  }
                  
                  
                  
                  inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                  }
                  
                  
                  
                  inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | insideOutsideDomain << (4));
                  }
                  
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const InsideOutsideDomain& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getInsideOutsideDomainMapping();
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const RefinementControl& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getRefinementControlMapping();
                  
                  /**
                   * Generated
                   */
                  std::string toString() const;
                  
                  /**
                   * Generated
                   */
                  void toString(std::ostream& out) const;
                  
                  
                  PersistentRecords getPersistentRecords() const;
                  /**
                   * Generated
                   */
                  Vertex convert() const;
                  
                  
               #ifdef Parallel
                  protected:
                     static tarch::logging::Log _log;
                     
                     int _senderDestinationRank;
                     
                  public:
                     
                     /**
                      * Global that represents the mpi datatype.
                      * There are two variants: Datatype identifies only those attributes marked with
                      * parallelise. FullDatatype instead identifies the whole record with all fields.
                      */
                     static MPI_Datatype Datatype;
                     static MPI_Datatype FullDatatype;
                     
                     /**
                      * Initializes the data type for the mpi operations. Has to be called
                      * before the very first send or receive operation is called.
                      */
                     static void initDatatype();
                     
                     static void shutdownDatatype();
                     
                     void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     int getSenderRank() const;
                     
               #endif
                  
               };
               
               #ifdef PackedRecords
               #pragma pack (pop)
               #endif
               
               
               
            
         #elif !defined(Parallel) && defined(Asserts)
            /**
             * @author This class is generated by DaStGen
             * 		   DataStructureGenerator (DaStGen)
             * 		   2007-2009 Wolfgang Eckhardt
             * 		   2012      Tobias Weinzierl
             *
             * 		   build date: 20-02-2013 11:21
             *
             * @date   02/04/2013 15:53
             */
            class rotatingheatsource::expliciteuler::records::Vertex { 
               
               public:
                  
                  typedef rotatingheatsource::expliciteuler::records::VertexPacked Packed;
                  
                  enum InsideOutsideDomain {
                     Inside = 0, Boundary = 1, Outside = 2
                  };
                  
                  enum RefinementControl {
                     Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6
                  };
                  
                  struct PersistentRecords {
                     double _rhs;
                     double _u;
                     #ifdef UseManualAlignment
                     tarch::la::Vector<THREE_POWER_D,double> _stencil __attribute__((aligned(VectorisationAlignment)));
                     #else
                     tarch::la::Vector<THREE_POWER_D,double> _stencil;
                     #endif
                     #ifdef UseManualAlignment
                     tarch::la::Vector<DIMENSIONS,double> _linearSurplus __attribute__((aligned(VectorisationAlignment)));
                     #else
                     tarch::la::Vector<DIMENSIONS,double> _linearSurplus;
                     #endif
                     bool _isHangingNode;
                     RefinementControl _refinementControl;
                     int _adjacentCellsHeight;
                     InsideOutsideDomain _insideOutsideDomain;
                     #ifdef UseManualAlignment
                     tarch::la::Vector<DIMENSIONS,double> _x __attribute__((aligned(VectorisationAlignment)));
                     #else
                     tarch::la::Vector<DIMENSIONS,double> _x;
                     #endif
                     int _level;
                     /**
                      * Generated
                      */
                     PersistentRecords();
                     
                     /**
                      * Generated
                      */
                     PersistentRecords(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                     
                     
                     inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _rhs;
                     }
                     
                     
                     
                     inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _rhs = rhs;
                     }
                     
                     
                     
                     inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _u;
                     }
                     
                     
                     
                     inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _u = u;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _stencil;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _stencil = (stencil);
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _linearSurplus;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _linearSurplus = (linearSurplus);
                     }
                     
                     
                     
                     inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _isHangingNode;
                     }
                     
                     
                     
                     inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _isHangingNode = isHangingNode;
                     }
                     
                     
                     
                     inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _refinementControl;
                     }
                     
                     
                     
                     inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _refinementControl = refinementControl;
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _adjacentCellsHeight;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _adjacentCellsHeight = adjacentCellsHeight;
                     }
                     
                     
                     
                     inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _insideOutsideDomain;
                     }
                     
                     
                     
                     inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _insideOutsideDomain = insideOutsideDomain;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _x;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _x = (x);
                     }
                     
                     
                     
                     inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _level;
                     }
                     
                     
                     
                     inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _level = level;
                     }
                     
                     
                     
                  };
                  
               private: 
                  PersistentRecords _persistentRecords;
                  double _residual;
                  int _adjacentCellsHeightOfPreviousIteration;
                  int _numberOfAdjacentRefinedCells;
                  
               public:
                  /**
                   * Generated
                   */
                  Vertex();
                  
                  /**
                   * Generated
                   */
                  Vertex(const PersistentRecords& persistentRecords);
                  
                  /**
                   * Generated
                   */
                  Vertex(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                  
                  /**
                   * Generated
                   */
                  Vertex(const double& rhs, const double& u, const double& residual, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                  
                  /**
                   * Generated
                   */
                  virtual ~Vertex();
                  
                  
                  inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._rhs;
                  }
                  
                  
                  
                  inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._rhs = rhs;
                  }
                  
                  
                  
                  inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._u;
                  }
                  
                  
                  
                  inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._u = u;
                  }
                  
                  
                  
                  inline double getResidual() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _residual;
                  }
                  
                  
                  
                  inline void setResidual(const double& residual) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _residual = residual;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._stencil;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._stencil = (stencil);
                  }
                  
                  
                  
                  inline double getStencil(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<THREE_POWER_D);
                     return _persistentRecords._stencil[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setStencil(int elementIndex, const double& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<THREE_POWER_D);
                     _persistentRecords._stencil[elementIndex]= stencil;
                     
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._linearSurplus;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._linearSurplus = (linearSurplus);
                  }
                  
                  
                  
                  inline double getLinearSurplus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._linearSurplus[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setLinearSurplus(int elementIndex, const double& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._linearSurplus[elementIndex]= linearSurplus;
                     
                  }
                  
                  
                  
                  inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._isHangingNode;
                  }
                  
                  
                  
                  inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._isHangingNode = isHangingNode;
                  }
                  
                  
                  
                  inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._refinementControl;
                  }
                  
                  
                  
                  inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._refinementControl = refinementControl;
                  }
                  
                  
                  
                  inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._adjacentCellsHeight;
                  }
                  
                  
                  
                  inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
                  }
                  
                  
                  
                  inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _adjacentCellsHeightOfPreviousIteration;
                  }
                  
                  
                  
                  inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
                  }
                  
                  
                  
                  inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _numberOfAdjacentRefinedCells;
                  }
                  
                  
                  
                  inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
                  }
                  
                  
                  
                  inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._insideOutsideDomain;
                  }
                  
                  
                  
                  inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._insideOutsideDomain = insideOutsideDomain;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._x;
                  }
                  
                  
                  
                  /**
                   * Generated and optimized
                   * 
                   * If you realise a for loop using exclusively arrays (vectors) and compile 
                   * with -DUseManualAlignment you may add 
                   * \code
                   #pragma vector aligned
                   #pragma simd
                   \endcode to this for loop to enforce your compiler to use SSE/AVX.
                   * 
                   * The alignment is tied to the unpacked records, i.e. for packed class
                   * variants the machine's natural alignment is switched off to recude the  
                   * memory footprint. Do not use any SSE/AVX operations or 
                   * vectorisation on the result for the packed variants, as the data is misaligned. 
                   * If you rely on vectorisation, convert the underlying record 
                   * into the unpacked version first. 
                   * 
                   * @see convert()
                   */
                  inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._x = (x);
                  }
                  
                  
                  
                  inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     return _persistentRecords._x[elementIndex];
                     
                  }
                  
                  
                  
                  inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     assertion(elementIndex>=0);
                     assertion(elementIndex<DIMENSIONS);
                     _persistentRecords._x[elementIndex]= x;
                     
                  }
                  
                  
                  
                  inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     return _persistentRecords._level;
                  }
                  
                  
                  
                  inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                     _persistentRecords._level = level;
                  }
                  
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const InsideOutsideDomain& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getInsideOutsideDomainMapping();
                  
                  /**
                   * Generated
                   */
                  static std::string toString(const RefinementControl& param);
                  
                  /**
                   * Generated
                   */
                  static std::string getRefinementControlMapping();
                  
                  /**
                   * Generated
                   */
                  std::string toString() const;
                  
                  /**
                   * Generated
                   */
                  void toString(std::ostream& out) const;
                  
                  
                  PersistentRecords getPersistentRecords() const;
                  /**
                   * Generated
                   */
                  VertexPacked convert() const;
                  
                  
               #ifdef Parallel
                  protected:
                     static tarch::logging::Log _log;
                     
                     int _senderDestinationRank;
                     
                  public:
                     
                     /**
                      * Global that represents the mpi datatype.
                      * There are two variants: Datatype identifies only those attributes marked with
                      * parallelise. FullDatatype instead identifies the whole record with all fields.
                      */
                     static MPI_Datatype Datatype;
                     static MPI_Datatype FullDatatype;
                     
                     /**
                      * Initializes the data type for the mpi operations. Has to be called
                      * before the very first send or receive operation is called.
                      */
                     static void initDatatype();
                     
                     static void shutdownDatatype();
                     
                     void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                     
                     int getSenderRank() const;
                     
               #endif
                  
               };
               
               #ifndef DaStGenPackedPadding
                 #define DaStGenPackedPadding 1      // 32 bit version
                 // #define DaStGenPackedPadding 2   // 64 bit version
               #endif
               
               
               #ifdef PackedRecords
                  #pragma pack (push, DaStGenPackedPadding)
               #endif
               
               /**
                * @author This class is generated by DaStGen
                * 		   DataStructureGenerator (DaStGen)
                * 		   2007-2009 Wolfgang Eckhardt
                * 		   2012      Tobias Weinzierl
                *
                * 		   build date: 20-02-2013 11:21
                *
                * @date   02/04/2013 15:53
                */
               class rotatingheatsource::expliciteuler::records::VertexPacked { 
                  
                  public:
                     
                     typedef rotatingheatsource::expliciteuler::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
                     
                     typedef rotatingheatsource::expliciteuler::records::Vertex::RefinementControl RefinementControl;
                     
                     struct PersistentRecords {
                        double _rhs;
                        double _u;
                        tarch::la::Vector<THREE_POWER_D,double> _stencil;
                        tarch::la::Vector<DIMENSIONS,double> _linearSurplus;
                        int _adjacentCellsHeight;
                        tarch::la::Vector<DIMENSIONS,double> _x;
                        int _level;
                        
                        /** mapping of records:
                        || Member 	|| startbit 	|| length
                         |  isHangingNode	| startbit 0	| #bits 1
                         |  refinementControl	| startbit 1	| #bits 3
                         |  insideOutsideDomain	| startbit 4	| #bits 2
                         */
                        short int _packedRecords0;
                        
                        /**
                         * Generated
                         */
                        PersistentRecords();
                        
                        /**
                         * Generated
                         */
                        PersistentRecords(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                        
                        
                        inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _rhs;
                        }
                        
                        
                        
                        inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _rhs = rhs;
                        }
                        
                        
                        
                        inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _u;
                        }
                        
                        
                        
                        inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _u = u;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _stencil;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _stencil = (stencil);
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _linearSurplus;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _linearSurplus = (linearSurplus);
                        }
                        
                        
                        
                        inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                        }
                        
                        
                        
                        inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                        }
                        
                        
                        
                        inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | refinementControl << (1));
                        }
                        
                        
                        
                        inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _adjacentCellsHeight;
                        }
                        
                        
                        
                        inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _adjacentCellsHeight = adjacentCellsHeight;
                        }
                        
                        
                        
                        inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                        }
                        
                        
                        
                        inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | insideOutsideDomain << (4));
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _x;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _x = (x);
                        }
                        
                        
                        
                        inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _level;
                        }
                        
                        
                        
                        inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _level = level;
                        }
                        
                        
                        
                     };
                     
                  private: 
                     PersistentRecords _persistentRecords;
                     double _residual;
                     int _adjacentCellsHeightOfPreviousIteration;
                     int _numberOfAdjacentRefinedCells;
                     
                  public:
                     /**
                      * Generated
                      */
                     VertexPacked();
                     
                     /**
                      * Generated
                      */
                     VertexPacked(const PersistentRecords& persistentRecords);
                     
                     /**
                      * Generated
                      */
                     VertexPacked(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                     
                     /**
                      * Generated
                      */
                     VertexPacked(const double& rhs, const double& u, const double& residual, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<DIMENSIONS,double>& x, const int& level);
                     
                     /**
                      * Generated
                      */
                     virtual ~VertexPacked();
                     
                     
                     inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._rhs;
                     }
                     
                     
                     
                     inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._rhs = rhs;
                     }
                     
                     
                     
                     inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._u;
                     }
                     
                     
                     
                     inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._u = u;
                     }
                     
                     
                     
                     inline double getResidual() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _residual;
                     }
                     
                     
                     
                     inline void setResidual(const double& residual) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _residual = residual;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._stencil;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._stencil = (stencil);
                     }
                     
                     
                     
                     inline double getStencil(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<THREE_POWER_D);
                        return _persistentRecords._stencil[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setStencil(int elementIndex, const double& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<THREE_POWER_D);
                        _persistentRecords._stencil[elementIndex]= stencil;
                        
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._linearSurplus;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._linearSurplus = (linearSurplus);
                     }
                     
                     
                     
                     inline double getLinearSurplus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        return _persistentRecords._linearSurplus[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setLinearSurplus(int elementIndex, const double& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        _persistentRecords._linearSurplus[elementIndex]= linearSurplus;
                        
                     }
                     
                     
                     
                     inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                     }
                     
                     
                     
                     inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                     }
                     
                     
                     
                     inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                     }
                     
                     
                     
                     inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | refinementControl << (1));
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._adjacentCellsHeight;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _adjacentCellsHeightOfPreviousIteration;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
                     }
                     
                     
                     
                     inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfAdjacentRefinedCells;
                     }
                     
                     
                     
                     inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
                     }
                     
                     
                     
                     inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                     }
                     
                     
                     
                     inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | insideOutsideDomain << (4));
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getX() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._x;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setX(const tarch::la::Vector<DIMENSIONS,double>& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._x = (x);
                     }
                     
                     
                     
                     inline double getX(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        return _persistentRecords._x[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setX(int elementIndex, const double& x) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        _persistentRecords._x[elementIndex]= x;
                        
                     }
                     
                     
                     
                     inline int getLevel() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._level;
                     }
                     
                     
                     
                     inline void setLevel(const int& level) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._level = level;
                     }
                     
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const InsideOutsideDomain& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getInsideOutsideDomainMapping();
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const RefinementControl& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getRefinementControlMapping();
                     
                     /**
                      * Generated
                      */
                     std::string toString() const;
                     
                     /**
                      * Generated
                      */
                     void toString(std::ostream& out) const;
                     
                     
                     PersistentRecords getPersistentRecords() const;
                     /**
                      * Generated
                      */
                     Vertex convert() const;
                     
                     
                  #ifdef Parallel
                     protected:
                        static tarch::logging::Log _log;
                        
                        int _senderDestinationRank;
                        
                     public:
                        
                        /**
                         * Global that represents the mpi datatype.
                         * There are two variants: Datatype identifies only those attributes marked with
                         * parallelise. FullDatatype instead identifies the whole record with all fields.
                         */
                        static MPI_Datatype Datatype;
                        static MPI_Datatype FullDatatype;
                        
                        /**
                         * Initializes the data type for the mpi operations. Has to be called
                         * before the very first send or receive operation is called.
                         */
                        static void initDatatype();
                        
                        static void shutdownDatatype();
                        
                        void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        int getSenderRank() const;
                        
                  #endif
                     
                  };
                  
                  #ifdef PackedRecords
                  #pragma pack (pop)
                  #endif
                  
                  
                  
               
            #elif defined(Parallel) && !defined(Asserts)
               /**
                * @author This class is generated by DaStGen
                * 		   DataStructureGenerator (DaStGen)
                * 		   2007-2009 Wolfgang Eckhardt
                * 		   2012      Tobias Weinzierl
                *
                * 		   build date: 20-02-2013 11:21
                *
                * @date   02/04/2013 15:53
                */
               class rotatingheatsource::expliciteuler::records::Vertex { 
                  
                  public:
                     
                     typedef rotatingheatsource::expliciteuler::records::VertexPacked Packed;
                     
                     enum InsideOutsideDomain {
                        Inside = 0, Boundary = 1, Outside = 2
                     };
                     
                     enum RefinementControl {
                        Unrefined = 0, Refined = 1, RefinementTriggered = 2, Refining = 3, EraseTriggered = 4, Erasing = 5, RefineDueToJoinThoughWorkerIsAlreadyErasing = 6
                     };
                     
                     struct PersistentRecords {
                        double _rhs;
                        double _u;
                        #ifdef UseManualAlignment
                        tarch::la::Vector<THREE_POWER_D,double> _stencil __attribute__((aligned(VectorisationAlignment)));
                        #else
                        tarch::la::Vector<THREE_POWER_D,double> _stencil;
                        #endif
                        #ifdef UseManualAlignment
                        tarch::la::Vector<DIMENSIONS,double> _linearSurplus __attribute__((aligned(VectorisationAlignment)));
                        #else
                        tarch::la::Vector<DIMENSIONS,double> _linearSurplus;
                        #endif
                        bool _isHangingNode;
                        RefinementControl _refinementControl;
                        int _adjacentCellsHeight;
                        InsideOutsideDomain _insideOutsideDomain;
                        #ifdef UseManualAlignment
                        tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks __attribute__((aligned(VectorisationAlignment)));
                        #else
                        tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
                        #endif
                        /**
                         * Generated
                         */
                        PersistentRecords();
                        
                        /**
                         * Generated
                         */
                        PersistentRecords(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
                        
                        
                        inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _rhs;
                        }
                        
                        
                        
                        inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _rhs = rhs;
                        }
                        
                        
                        
                        inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _u;
                        }
                        
                        
                        
                        inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _u = u;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _stencil;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _stencil = (stencil);
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _linearSurplus;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _linearSurplus = (linearSurplus);
                        }
                        
                        
                        
                        inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _isHangingNode;
                        }
                        
                        
                        
                        inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _isHangingNode = isHangingNode;
                        }
                        
                        
                        
                        inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _refinementControl;
                        }
                        
                        
                        
                        inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _refinementControl = refinementControl;
                        }
                        
                        
                        
                        inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _adjacentCellsHeight;
                        }
                        
                        
                        
                        inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _adjacentCellsHeight = adjacentCellsHeight;
                        }
                        
                        
                        
                        inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _insideOutsideDomain;
                        }
                        
                        
                        
                        inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _insideOutsideDomain = insideOutsideDomain;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _adjacentRanks;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _adjacentRanks = (adjacentRanks);
                        }
                        
                        
                        
                     };
                     
                  private: 
                     PersistentRecords _persistentRecords;
                     double _residual;
                     int _adjacentCellsHeightOfPreviousIteration;
                     int _numberOfAdjacentRefinedCells;
                     
                  public:
                     /**
                      * Generated
                      */
                     Vertex();
                     
                     /**
                      * Generated
                      */
                     Vertex(const PersistentRecords& persistentRecords);
                     
                     /**
                      * Generated
                      */
                     Vertex(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
                     
                     /**
                      * Generated
                      */
                     Vertex(const double& rhs, const double& u, const double& residual, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
                     
                     /**
                      * Generated
                      */
                     virtual ~Vertex();
                     
                     
                     inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._rhs;
                     }
                     
                     
                     
                     inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._rhs = rhs;
                     }
                     
                     
                     
                     inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._u;
                     }
                     
                     
                     
                     inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._u = u;
                     }
                     
                     
                     
                     inline double getResidual() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _residual;
                     }
                     
                     
                     
                     inline void setResidual(const double& residual) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _residual = residual;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._stencil;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._stencil = (stencil);
                     }
                     
                     
                     
                     inline double getStencil(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<THREE_POWER_D);
                        return _persistentRecords._stencil[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setStencil(int elementIndex, const double& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<THREE_POWER_D);
                        _persistentRecords._stencil[elementIndex]= stencil;
                        
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._linearSurplus;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._linearSurplus = (linearSurplus);
                     }
                     
                     
                     
                     inline double getLinearSurplus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        return _persistentRecords._linearSurplus[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setLinearSurplus(int elementIndex, const double& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<DIMENSIONS);
                        _persistentRecords._linearSurplus[elementIndex]= linearSurplus;
                        
                     }
                     
                     
                     
                     inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._isHangingNode;
                     }
                     
                     
                     
                     inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._isHangingNode = isHangingNode;
                     }
                     
                     
                     
                     inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._refinementControl;
                     }
                     
                     
                     
                     inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._refinementControl = refinementControl;
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._adjacentCellsHeight;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
                     }
                     
                     
                     
                     inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _adjacentCellsHeightOfPreviousIteration;
                     }
                     
                     
                     
                     inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
                     }
                     
                     
                     
                     inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _numberOfAdjacentRefinedCells;
                     }
                     
                     
                     
                     inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
                     }
                     
                     
                     
                     inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._insideOutsideDomain;
                     }
                     
                     
                     
                     inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._insideOutsideDomain = insideOutsideDomain;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        return _persistentRecords._adjacentRanks;
                     }
                     
                     
                     
                     /**
                      * Generated and optimized
                      * 
                      * If you realise a for loop using exclusively arrays (vectors) and compile 
                      * with -DUseManualAlignment you may add 
                      * \code
                      #pragma vector aligned
                      #pragma simd
                      \endcode to this for loop to enforce your compiler to use SSE/AVX.
                      * 
                      * The alignment is tied to the unpacked records, i.e. for packed class
                      * variants the machine's natural alignment is switched off to recude the  
                      * memory footprint. Do not use any SSE/AVX operations or 
                      * vectorisation on the result for the packed variants, as the data is misaligned. 
                      * If you rely on vectorisation, convert the underlying record 
                      * into the unpacked version first. 
                      * 
                      * @see convert()
                      */
                     inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        _persistentRecords._adjacentRanks = (adjacentRanks);
                     }
                     
                     
                     
                     inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        return _persistentRecords._adjacentRanks[elementIndex];
                        
                     }
                     
                     
                     
                     inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                        assertion(elementIndex>=0);
                        assertion(elementIndex<TWO_POWER_D);
                        _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
                        
                     }
                     
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const InsideOutsideDomain& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getInsideOutsideDomainMapping();
                     
                     /**
                      * Generated
                      */
                     static std::string toString(const RefinementControl& param);
                     
                     /**
                      * Generated
                      */
                     static std::string getRefinementControlMapping();
                     
                     /**
                      * Generated
                      */
                     std::string toString() const;
                     
                     /**
                      * Generated
                      */
                     void toString(std::ostream& out) const;
                     
                     
                     PersistentRecords getPersistentRecords() const;
                     /**
                      * Generated
                      */
                     VertexPacked convert() const;
                     
                     
                  #ifdef Parallel
                     protected:
                        static tarch::logging::Log _log;
                        
                        int _senderDestinationRank;
                        
                     public:
                        
                        /**
                         * Global that represents the mpi datatype.
                         * There are two variants: Datatype identifies only those attributes marked with
                         * parallelise. FullDatatype instead identifies the whole record with all fields.
                         */
                        static MPI_Datatype Datatype;
                        static MPI_Datatype FullDatatype;
                        
                        /**
                         * Initializes the data type for the mpi operations. Has to be called
                         * before the very first send or receive operation is called.
                         */
                        static void initDatatype();
                        
                        static void shutdownDatatype();
                        
                        void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                        
                        int getSenderRank() const;
                        
                  #endif
                     
                  };
                  
                  #ifndef DaStGenPackedPadding
                    #define DaStGenPackedPadding 1      // 32 bit version
                    // #define DaStGenPackedPadding 2   // 64 bit version
                  #endif
                  
                  
                  #ifdef PackedRecords
                     #pragma pack (push, DaStGenPackedPadding)
                  #endif
                  
                  /**
                   * @author This class is generated by DaStGen
                   * 		   DataStructureGenerator (DaStGen)
                   * 		   2007-2009 Wolfgang Eckhardt
                   * 		   2012      Tobias Weinzierl
                   *
                   * 		   build date: 20-02-2013 11:21
                   *
                   * @date   02/04/2013 15:53
                   */
                  class rotatingheatsource::expliciteuler::records::VertexPacked { 
                     
                     public:
                        
                        typedef rotatingheatsource::expliciteuler::records::Vertex::InsideOutsideDomain InsideOutsideDomain;
                        
                        typedef rotatingheatsource::expliciteuler::records::Vertex::RefinementControl RefinementControl;
                        
                        struct PersistentRecords {
                           double _rhs;
                           double _u;
                           tarch::la::Vector<THREE_POWER_D,double> _stencil;
                           tarch::la::Vector<DIMENSIONS,double> _linearSurplus;
                           int _adjacentCellsHeight;
                           tarch::la::Vector<TWO_POWER_D,int> _adjacentRanks;
                           
                           /** mapping of records:
                           || Member 	|| startbit 	|| length
                            |  isHangingNode	| startbit 0	| #bits 1
                            |  refinementControl	| startbit 1	| #bits 3
                            |  insideOutsideDomain	| startbit 4	| #bits 2
                            */
                           short int _packedRecords0;
                           
                           /**
                            * Generated
                            */
                           PersistentRecords();
                           
                           /**
                            * Generated
                            */
                           PersistentRecords(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
                           
                           
                           inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _rhs;
                           }
                           
                           
                           
                           inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _rhs = rhs;
                           }
                           
                           
                           
                           inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _u;
                           }
                           
                           
                           
                           inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _u = u;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _stencil;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _stencil = (stencil);
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _linearSurplus;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _linearSurplus = (linearSurplus);
                           }
                           
                           
                           
                           inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   return (tmp != 0);
                           }
                           
                           
                           
                           inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask = 1 << (0);
   _packedRecords0 = static_cast<short int>( isHangingNode ? (_packedRecords0 | mask) : (_packedRecords0 & ~mask));
                           }
                           
                           
                           
                           inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                           }
                           
                           
                           
                           inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | refinementControl << (1));
                           }
                           
                           
                           
                           inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _adjacentCellsHeight;
                           }
                           
                           
                           
                           inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _adjacentCellsHeight = adjacentCellsHeight;
                           }
                           
                           
                           
                           inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   short int tmp = static_cast<short int>(_packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                           }
                           
                           
                           
                           inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   _packedRecords0 = static_cast<short int>(_packedRecords0 & ~mask);
   _packedRecords0 = static_cast<short int>(_packedRecords0 | insideOutsideDomain << (4));
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              return _adjacentRanks;
                           }
                           
                           
                           
                           /**
                            * Generated and optimized
                            * 
                            * If you realise a for loop using exclusively arrays (vectors) and compile 
                            * with -DUseManualAlignment you may add 
                            * \code
                            #pragma vector aligned
                            #pragma simd
                            \endcode to this for loop to enforce your compiler to use SSE/AVX.
                            * 
                            * The alignment is tied to the unpacked records, i.e. for packed class
                            * variants the machine's natural alignment is switched off to recude the  
                            * memory footprint. Do not use any SSE/AVX operations or 
                            * vectorisation on the result for the packed variants, as the data is misaligned. 
                            * If you rely on vectorisation, convert the underlying record 
                            * into the unpacked version first. 
                            * 
                            * @see convert()
                            */
                           inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                              _adjacentRanks = (adjacentRanks);
                           }
                           
                           
                           
                        };
                        
                     private: 
                        PersistentRecords _persistentRecords;
                        double _residual;
                        int _adjacentCellsHeightOfPreviousIteration;
                        int _numberOfAdjacentRefinedCells;
                        
                     public:
                        /**
                         * Generated
                         */
                        VertexPacked();
                        
                        /**
                         * Generated
                         */
                        VertexPacked(const PersistentRecords& persistentRecords);
                        
                        /**
                         * Generated
                         */
                        VertexPacked(const double& rhs, const double& u, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
                        
                        /**
                         * Generated
                         */
                        VertexPacked(const double& rhs, const double& u, const double& residual, const tarch::la::Vector<THREE_POWER_D,double>& stencil, const tarch::la::Vector<DIMENSIONS,double>& linearSurplus, const bool& isHangingNode, const RefinementControl& refinementControl, const int& adjacentCellsHeight, const int& adjacentCellsHeightOfPreviousIteration, const int& numberOfAdjacentRefinedCells, const InsideOutsideDomain& insideOutsideDomain, const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks);
                        
                        /**
                         * Generated
                         */
                        virtual ~VertexPacked();
                        
                        
                        inline double getRhs() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._rhs;
                        }
                        
                        
                        
                        inline void setRhs(const double& rhs) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._rhs = rhs;
                        }
                        
                        
                        
                        inline double getU() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._u;
                        }
                        
                        
                        
                        inline void setU(const double& u) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._u = u;
                        }
                        
                        
                        
                        inline double getResidual() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _residual;
                        }
                        
                        
                        
                        inline void setResidual(const double& residual) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _residual = residual;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<THREE_POWER_D,double> getStencil() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._stencil;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setStencil(const tarch::la::Vector<THREE_POWER_D,double>& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._stencil = (stencil);
                        }
                        
                        
                        
                        inline double getStencil(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<THREE_POWER_D);
                           return _persistentRecords._stencil[elementIndex];
                           
                        }
                        
                        
                        
                        inline void setStencil(int elementIndex, const double& stencil) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<THREE_POWER_D);
                           _persistentRecords._stencil[elementIndex]= stencil;
                           
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<DIMENSIONS,double> getLinearSurplus() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._linearSurplus;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setLinearSurplus(const tarch::la::Vector<DIMENSIONS,double>& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._linearSurplus = (linearSurplus);
                        }
                        
                        
                        
                        inline double getLinearSurplus(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           return _persistentRecords._linearSurplus[elementIndex];
                           
                        }
                        
                        
                        
                        inline void setLinearSurplus(int elementIndex, const double& linearSurplus) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<DIMENSIONS);
                           _persistentRecords._linearSurplus[elementIndex]= linearSurplus;
                           
                        }
                        
                        
                        
                        inline bool getIsHangingNode() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   return (tmp != 0);
                        }
                        
                        
                        
                        inline void setIsHangingNode(const bool& isHangingNode) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask = 1 << (0);
   _persistentRecords._packedRecords0 = static_cast<short int>( isHangingNode ? (_persistentRecords._packedRecords0 | mask) : (_persistentRecords._packedRecords0 & ~mask));
                        }
                        
                        
                        
                        inline RefinementControl getRefinementControl() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (1));
   assertion(( tmp >= 0 &&  tmp <= 6));
   return (RefinementControl) tmp;
                        }
                        
                        
                        
                        inline void setRefinementControl(const RefinementControl& refinementControl) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((refinementControl >= 0 && refinementControl <= 6));
   short int mask =  (1 << (3)) - 1;
   mask = static_cast<short int>(mask << (1));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | refinementControl << (1));
                        }
                        
                        
                        
                        inline int getAdjacentCellsHeight() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._adjacentCellsHeight;
                        }
                        
                        
                        
                        inline void setAdjacentCellsHeight(const int& adjacentCellsHeight) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._adjacentCellsHeight = adjacentCellsHeight;
                        }
                        
                        
                        
                        inline int getAdjacentCellsHeightOfPreviousIteration() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _adjacentCellsHeightOfPreviousIteration;
                        }
                        
                        
                        
                        inline void setAdjacentCellsHeightOfPreviousIteration(const int& adjacentCellsHeightOfPreviousIteration) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _adjacentCellsHeightOfPreviousIteration = adjacentCellsHeightOfPreviousIteration;
                        }
                        
                        
                        
                        inline int getNumberOfAdjacentRefinedCells() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _numberOfAdjacentRefinedCells;
                        }
                        
                        
                        
                        inline void setNumberOfAdjacentRefinedCells(const int& numberOfAdjacentRefinedCells) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _numberOfAdjacentRefinedCells = numberOfAdjacentRefinedCells;
                        }
                        
                        
                        
                        inline InsideOutsideDomain getInsideOutsideDomain() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   short int tmp = static_cast<short int>(_persistentRecords._packedRecords0 & mask);
   tmp = static_cast<short int>(tmp >> (4));
   assertion(( tmp >= 0 &&  tmp <= 2));
   return (InsideOutsideDomain) tmp;
                        }
                        
                        
                        
                        inline void setInsideOutsideDomain(const InsideOutsideDomain& insideOutsideDomain) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion((insideOutsideDomain >= 0 && insideOutsideDomain <= 2));
   short int mask =  (1 << (2)) - 1;
   mask = static_cast<short int>(mask << (4));
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 & ~mask);
   _persistentRecords._packedRecords0 = static_cast<short int>(_persistentRecords._packedRecords0 | insideOutsideDomain << (4));
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline tarch::la::Vector<TWO_POWER_D,int> getAdjacentRanks() const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           return _persistentRecords._adjacentRanks;
                        }
                        
                        
                        
                        /**
                         * Generated and optimized
                         * 
                         * If you realise a for loop using exclusively arrays (vectors) and compile 
                         * with -DUseManualAlignment you may add 
                         * \code
                         #pragma vector aligned
                         #pragma simd
                         \endcode to this for loop to enforce your compiler to use SSE/AVX.
                         * 
                         * The alignment is tied to the unpacked records, i.e. for packed class
                         * variants the machine's natural alignment is switched off to recude the  
                         * memory footprint. Do not use any SSE/AVX operations or 
                         * vectorisation on the result for the packed variants, as the data is misaligned. 
                         * If you rely on vectorisation, convert the underlying record 
                         * into the unpacked version first. 
                         * 
                         * @see convert()
                         */
                        inline void setAdjacentRanks(const tarch::la::Vector<TWO_POWER_D,int>& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           _persistentRecords._adjacentRanks = (adjacentRanks);
                        }
                        
                        
                        
                        inline int getAdjacentRanks(int elementIndex) const 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<TWO_POWER_D);
                           return _persistentRecords._adjacentRanks[elementIndex];
                           
                        }
                        
                        
                        
                        inline void setAdjacentRanks(int elementIndex, const int& adjacentRanks) 
 #ifdef UseManualInlining
 __attribute__((always_inline))
 #endif 
 {
                           assertion(elementIndex>=0);
                           assertion(elementIndex<TWO_POWER_D);
                           _persistentRecords._adjacentRanks[elementIndex]= adjacentRanks;
                           
                        }
                        
                        
                        /**
                         * Generated
                         */
                        static std::string toString(const InsideOutsideDomain& param);
                        
                        /**
                         * Generated
                         */
                        static std::string getInsideOutsideDomainMapping();
                        
                        /**
                         * Generated
                         */
                        static std::string toString(const RefinementControl& param);
                        
                        /**
                         * Generated
                         */
                        static std::string getRefinementControlMapping();
                        
                        /**
                         * Generated
                         */
                        std::string toString() const;
                        
                        /**
                         * Generated
                         */
                        void toString(std::ostream& out) const;
                        
                        
                        PersistentRecords getPersistentRecords() const;
                        /**
                         * Generated
                         */
                        Vertex convert() const;
                        
                        
                     #ifdef Parallel
                        protected:
                           static tarch::logging::Log _log;
                           
                           int _senderDestinationRank;
                           
                        public:
                           
                           /**
                            * Global that represents the mpi datatype.
                            * There are two variants: Datatype identifies only those attributes marked with
                            * parallelise. FullDatatype instead identifies the whole record with all fields.
                            */
                           static MPI_Datatype Datatype;
                           static MPI_Datatype FullDatatype;
                           
                           /**
                            * Initializes the data type for the mpi operations. Has to be called
                            * before the very first send or receive operation is called.
                            */
                           static void initDatatype();
                           
                           static void shutdownDatatype();
                           
                           void send(int destination, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           void receive(int source, int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           static bool isMessageInQueue(int tag, bool exchangeOnlyAttributesMarkedWithParallelise);
                           
                           int getSenderRank() const;
                           
                     #endif
                        
                     };
                     
                     #ifdef PackedRecords
                     #pragma pack (pop)
                     #endif
                     
                     
                     
                  
               #endif
               
               #endif
               
